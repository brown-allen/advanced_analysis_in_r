---
title: "Advanced Data Analysis in R"
subtitle: "Bayesian Modeling in R"
author: "Michael DeWitt"
date: "2018-03-17 (Updated `r Sys.Date()`)"
output:
  beamer_presentation:
    keep_tex: false
    theme: metropolis
    slide_level: 2
    incremental: false
    includes:
      in_header: head.txt
fontsize: 10pt
classoption: compress
bibliography: my_bib.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  echo = TRUE,
  cache = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.align = "center"
)
library(tidyverse)
library(broom)
library(here)
set.seed(27)
options(
  tibble.print_max = 3,
  tibble.print_min = 3,
  width = 55
)
theme_set(theme_gray(base_size = 22))
```

# Bayesian Modeling in R

## A Thought Exercise

You are already Bayesian!  

You just didn't know it!

## A Coin

What is the probability a given coin is fair?

## Frequentist

If you didn't answer 100% or 0% you're Bayesian!

## What is Bayes?

- Named after Rev. Thomas Bayes

```{r eval=FALSE}
knitr::include_graphics(here::here("bayesian_modeling", "figs", "bayes.png"))
```


## Bayes Theorem

Just derrived from identities of probability

$$P(A|B) = \frac{P(A\cap B)}{P(B)}$$

$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$


# Prior, Likelihood, Posterior Distribution

## Prior

Prior is _subjective_ and represents a range of potential values 

Specified by a distribution  

### Informative

Restricts range of likely values to small range

More informative adds more "weight" to the prior vs the data  

### Noninformative/ Uninformative

Wider range of possibilities  

More closely approximates the Maximum likelihood estimates  

## Likelihood

Likelihood is the distribution for the data generating process  

Examples 

- Poisson process -> poisson likelihood function  
- Binomial process -> binomial likelihood function 
- Normal distibution -> normal likelihood function  
- Ordered categorical -> ordered categorical likelihood function

## Posterior

$$P(A|B) \sim Prior * Likelihood$$

```{r}
curve(expr = dbeta(x, shape1 = .1, shape2 = 50), from = 0, to = 1)
curve(expr = dbinom(x, size = 100, prob = .5), from = 0, to = 100)
curve(expr = dbeta(x,shape1 = .1 + 50, shape2 = 50+50 ), from = 0, to = 1)
```


## Frequentist vs Bayesian Inferences

# Bayesian Workflow

## Doing Bayesian Inferences

## Create A Data Generating Process

Define your data generating process

Generate some fake data from it

## Write Your Model

Test it! Write a model, run it and test it.

## Domain Specific Languages

BUGS  
JAGS  
Stan  
Hand coded samplers 


## Enter `brms`

`brms` makes Bayesian Modeling Easy  

Utilises Hamiltonian Monte Carlo  with a No U-Turn Sampler


## Specifying a Model in `brms`

## Model Family

## Run the Model

## Posterior Checks

- Convergence  
  - Trace Plots `tracplot`
  - Rhat metrics
  - Effective Sample Size
  
- Posterior Predictive Intervals
  - Was there a good fit between the model and the data
  
## Inferences

## Advantages of Bayesian Analysis

- Takes advantage of expert opinion  
  - Especially helpful with small samples size studies  
  - Reduces possibility of wildly odd results  

- Easier communications (more intuitive to discuss probabilities)  

- Studies can build on one another
  - Results from one study can be supplied directly as a prior into a replciation or another study

## Drawbacks of Bayesian Inference

- Not as widely utilised in major publications  

- Computationally intensive 

- Picking a prior  

- Heuristics [exist](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations)


## References

<!--
- Post-stratification and Ranking
- Trimming Weights
- Cluster
-->