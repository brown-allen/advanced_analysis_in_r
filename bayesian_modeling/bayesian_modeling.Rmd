---
title: "Advanced Data Analysis in R"
subtitle: "Bayesian Modeling in R"
author: "Michael DeWitt"
date: "2018-03-17 (Updated `r Sys.Date()`)"
output:
  beamer_presentation:
    keep_tex: false
    theme: metropolis
    slide_level: 2
    incremental: false
    includes:
      in_header: head.txt
fontsize: 10pt
classoption: compress
bibliography: my_bib.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  echo = TRUE,
  cache = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.align = "center"
)
library(tidyverse)
library(broom)
library(here)
set.seed(27)
options(
  tibble.print_max = 3,
  tibble.print_min = 3,
  width = 55
)
theme_set(theme_gray(base_size = 22))
```

# Bayesian Modeling in R

## A Thought Exercise

You are already Bayesian!  

You just didn't know it!

## A Coin

What is the probability a given coin is fair?

## Frequentist

If you didn't answer 100% or 0% you're Bayesian!

## What is Bayes?

- Named after Rev. Thomas Bayes

```{r eval=FALSE}
knitr::include_graphics(here::here("bayesian_modeling", "figs", "bayes.png"))
```


## Bayes Theorem

Just derrived from identities of probability

$$P(A|B) = \frac{P(A\cap B)}{P(B)}$$

$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$


# Prior, Likelihood, Posterior Distribution

## Prior

Prior is _subjective_ and represents a range of potential values 

Specified by a distribution  

### Informative

Restricts range of likely values to small range

More informative adds more "weight" to the prior vs the data  

### Noninformative/ Uninformative

Wider range of possibilities  

More closely approximates the Maximum likelihood estimates  

## Likelihood

Likelihood is the distribution for the data generating process  

Examples 

- Poisson process -> poisson likelihood function  
- Binomial process -> binomial likelihood function 
- Normal distibution -> normal likelihood function  
- Ordered categorical -> ordered categorical likelihood function

## Posterior

$$P(A|B) \sim Prior * Likelihood$$

```{r}
curve(expr = dbeta(x, shape1 = .1, shape2 = 50), from = 0, to = 1)
curve(expr = dbinom(x, size = 100, prob = .5), from = 0, to = 100)
curve(expr = dbeta(x,shape1 = .1 + 50, shape2 = 50+50 ), from = 0, to = 1)
```


# Bayesian Workflow

## Doing Bayesian Inferences

Bayesian inference and modeling techniques can be applied across the board  

In MLE approachs you often make assumptions without even realising it!  

Bayes requires you to be more deliberate  

## Write Your Model

What is your data generating process?  

> We are estimating the support for a given referendum. Thus our population has a choice, either 1 (support) or 0 (do not support). We do not have any good estimates from previous literature for overall support. 

## Write The DGP in Math

### Likelihood Function
Series of bernouilli trials -> binomial likelihood function  
$$P(\theta| Data) \sim Binomial(Data, \theta)$$

### Prior

Additionally say that we have some data on

$$\theta \sim Beta(1,1)$$

## Simulate Your Data Generating Process

```{r}
n <- 1000
gender <-rep(x = 0:1, length.out = n)
income <- rnorm(n, 0, 1)
mu <- gender * 1.5 + income * 2
y <- rbinom(n,1, prob = plogis(mu))

dat <- data.frame(gender =gender, income = income, y = y)
```


## Domain Specific Languages

BUGS  
JAGS  
Stan  
Hand coded samplers 


## Enter `brms`

`brms` makes Bayesian Modeling Easy  

Utilises Hamiltonian Monte Carlo with a No U-Turn Sampler


## Specifying a Model in `brms`

```{r}
library(brms)
(model <- bf(y ~ gender + income))
```

## Inspect Priors

```{r}
get_prior(model, dat, bernoulli())
```

## Specify Priors

```{r}
my_priors <- c(
  prior(normal(0, 0.5), class = "b", coef = "gender"),
  prior(normal(0, 0.5), class = "b", coef = "income"))
```


## Model Family

```{r}
fit <- brm(model, my_priors,
           data =  dat, 
           family = bernoulli(), 
           inits = 1000, cores = 2, 
           chains = 2, seed = 1234, refresh = 0)
```

## Posterior Checks

- Convergence  
  - Trace Plots `tracplot`
  - Rhat metrics
  - Effective Sample Size
  
- Posterior Predictive Intervals
  - Was there a good fit between the model and the data
  
## So Let's Check

```{r out.width="400px"}
library(tidybayes)
library(bayesplot)
mcmc_trace(as.matrix(fit))
```


  
## Inferences

```{r}
summary(fit)
```


## Advantages of Bayesian Analysis

- Takes advantage of expert opinion  
  - Especially helpful with small samples size studies  
  - Reduces possibility of wildly odd results  

- Easier communications (more intuitive to discuss probabilities)  

- Studies can build on one another
  - Results from one study can be supplied directly as a prior into a replciation or another study

## Drawbacks of Bayesian Inference

- Not as widely utilised in major publications  

- Computationally intensive 

- Picking a prior  

- Heuristics [exist](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations)


## References

<!--
- Post-stratification and Ranking
- Trimming Weights
- Cluster
-->